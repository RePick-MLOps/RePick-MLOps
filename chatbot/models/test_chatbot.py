from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain.retrievers import BM25Retriever, EnsembleRetriever
from langchain_core.documents import Document
import chromadb
from chromadb.config import Settings
import logging
import subprocess
import os
from dotenv import load_dotenv

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()


def load_embedding_model(model_name="jhgan/ko-sbert-sts"):
    return HuggingFaceEmbeddings(model_name=model_name)


def load_vectorstore(vectordb_path=None):
    try:
        embeddings = load_embedding_model()
        local_db_path = "./data/vectordb"
        s3_bucket = "repick-chromadb"

        logger.info(f"ChromaDB ê²½ë¡œ: {local_db_path}")

        # S3ì—ì„œ ì „ì²´ ë°ì´í„° ë‹¤ìš´ë¡œë“œ
        try:

            # ì „ì²´ ë””ë ‰í† ë¦¬ ë™ê¸°í™”
            sync_cmd = f"aws s3 sync s3://repick-chromadb/vectordb {local_db_path}"
            logger.info(f"ì‹¤í–‰ ëª…ë ¹ì–´: {sync_cmd}")
            result = subprocess.run(
                sync_cmd, shell=True, capture_output=True, text=True
            )

            if result.returncode == 0:
                logger.info("S3ì—ì„œ ëª¨ë“  ë°ì´í„°ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            else:
                logger.error(f"S3 ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {result.stderr}")

            # ë¡œì»¬ ë””ë ‰í† ë¦¬ ë‚´ìš© í™•ì¸
            logger.info(f"ë¡œì»¬ ë””ë ‰í† ë¦¬ ë‚´ìš©:")
            ls_result = subprocess.run(
                f"ls -la {local_db_path}", shell=True, capture_output=True, text=True
            )
            logger.info(ls_result.stdout)

        except Exception as e:
            logger.error(f"S3 ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")

        # ChromaDB í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        client = chromadb.PersistentClient(
            path=local_db_path,
            settings=Settings(
                anonymized_telemetry=False, allow_reset=True, is_persistent=True
            ),
        )

        # Langchain Chroma ì´ˆê¸°í™”
        vectorstore = Chroma(
            client=client,
            embedding_function=embeddings,
            collection_name="pdf_collection",
        )

        # ì»¬ë ‰ì…˜ ìƒíƒœ í™•ì¸
        try:
            collection = client.get_collection("pdf_collection")
            count = collection.count()
            logger.info(f"\n=== Chroma DB ìƒíƒœ ===")
            logger.info(f"ì´ ë¬¸ì„œ ìˆ˜: {count}")
            logger.info(f"ì»¬ë ‰ì…˜ ì´ë¦„: pdf_collection")
            logger.info(f"ë¡œì»¬ ê²½ë¡œ: {local_db_path}")
        except Exception as e:
            logger.warning(f"ì»¬ë ‰ì…˜ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {str(e)}")
            collection = client.create_collection("pdf_collection")
            logger.info("ìƒˆ ì»¬ë ‰ì…˜ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

        return vectorstore

    except Exception as e:
        logger.error(f"Error loading vector store: {str(e)}")
        raise


def save_to_s3():
    """ChromaDB ë°ì´í„°ë¥¼ S3ì— ì—…ë¡œë“œ"""
    try:
        local_db_path = "data/vectordb"
        os.system(f"aws s3 sync {local_db_path} s3://repick-chromadb/vectordb")
        logger.info("ChromaDB ë°ì´í„°ë¥¼ S3ì— ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"S3 ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise


def create_prompt():
    return PromptTemplate.from_template(
        """You are an assistant for question-answering tasks. 
Use the following pieces of retrieved context to answer the question. 
If you don't know the answer, just say that you don't know. 
Answer in Korean.

# Direction:
Make sure you understand the intent of the question and provide the most appropriate answer.
- Ask yourself the context of the question and why the questioner asked it, think about the question, and provide an appropriate answer based on your understanding.
2. Select the most relevant content (the key content that directly relates to the question) from the context in which it was retrieved to write your answer.
3. Create a concise and logical answer. When creating your answer, don't just list your selections, but rearrange them to fit the context so they flow naturally into paragraphs.
4. If you haven't searched for context for the question, or if you've searched for a document but its content isn't relevant to the question, you should say 'I can't find an answer to that question in the materials I have'.
5. Write your answer in a table of key points.
6. Your answer must include all sources and page numbers.
7. Your answer must be written in Korean.
8. Be as detailed as possible in your answer.
9. Begin your answer with 'This answer is based on content found in the document **' and end with '**ğŸ“Œ source**'.
10. Page numbers should be whole numbers.

#Context: 
{context}

###

#Example Format:
**ğŸ“š ë¦¬í¬íŠ¸ì—ì„œ ê²€ìƒ‰í•œ ë‚´ìš© ê¸°ë°˜ì˜ ë‹µë³€ì…ë‹ˆë‹¤**

(brief summary of the answer)
(include table if there is a table in the context related to the question)
(include image explanation if there is a image in the context related to the question)
(detailed answer to the question)

**ğŸ“Œ ì¶œì²˜**
[here you only write filename(.pdf only), page]

- íŒŒì¼ëª….pdf, 192ìª½
- íŒŒì¼ëª….pdf, 192ìª½
- ...

###

#Question:
{question}

#Answer:"""
    )


def create_chain(retriever):
    llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)

    def retrieve_and_format(question: str) -> str:
        try:
            # ë¬¸ì„œ ê²€ìƒ‰
            docs = retriever.get_relevant_documents(question)

            # ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ë‚´ìš©ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©
            context = "\n\n".join(doc.page_content for doc in docs)

            # ë©”íƒ€ë°ì´í„° ì •ë³´ ì¶”ê°€
            sources = []
            for doc in docs:
                if doc.metadata.get("source") and doc.metadata.get("page"):
                    sources.append(
                        f"- {doc.metadata['source']}, {doc.metadata['page']}ìª½"
                    )

            if sources:
                context += "\n\nì¶œì²˜:\n" + "\n".join(sources)

            return context

        except Exception as e:
            print(f"Error in retrieve_and_format: {str(e)}")
            return "ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    chain = (
        {
            "context": RunnableLambda(retrieve_and_format),
            "question": RunnablePassthrough(),
        }
        | create_prompt()
        | llm
        | StrOutputParser()
    )

    return chain


def initialize_retrievers(vectorstore):
    # Chroma ë¦¬íŠ¸ë¦¬ë²„ ì´ˆê¸°í™”
    chroma_retriever = vectorstore.as_retriever(
        search_type="similarity", search_kwargs={"k": 5}
    )

    # BM25 ë¦¬íŠ¸ë²„ ì´ˆê¸°í™” (Chromaì—ì„œ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°)
    documents = vectorstore.get()  # Chromaì—ì„œ ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
    bm25_retriever = BM25Retriever.from_documents(documents)
    bm25_retriever.k = 5

    # ì•™ìƒë¸” ë¦¬íŠ¸ë¦¬ë²„ ì´ˆê¸°í™”
    ensemble_retriever = EnsembleRetriever(
        retrievers=[chroma_retriever, bm25_retriever], weights=[0.7, 0.3]
    )

    return ensemble_retriever


def clean_retrieved_documents(retrieved_documents):
    clean_docs = []

    for doc in retrieved_documents:
        # Document ê°ì²´ê°€ ì•„ë‹Œ ê²½ìš° ê±´ë„ˆë›°ê¸°
        if not hasattr(doc, "metadata") or not hasattr(doc, "page_content"):
            continue

        metadata = doc.metadata
        new_metadata = {}
        content = doc.page_content

        if isinstance(content, dict):
            # contentê°€ dictì¸ ê²½ìš° ë¬¸ìì—´ë¡œ ë³€í™˜
            content = str(content)

        if metadata.get("type") in ["page_summary", "text"]:
            if "page" in metadata:
                new_metadata["page"] = metadata["page"]
            if "source" in metadata:
                new_metadata["source"] = metadata["source"]
            if metadata.get("type") == "text" and "summary" in metadata:
                new_metadata["summary"] = metadata["summary"]
            clean_docs.append(Document(page_content=content, metadata=new_metadata))

        elif metadata.get("type") == "hypothetical_questions":
            content = metadata.get("summary", content)
            if "page" in metadata:
                new_metadata["page"] = metadata["page"]
            if "source" in metadata:
                new_metadata["source"] = metadata["source"]
            clean_docs.append(Document(page_content=content, metadata=new_metadata))

    return clean_docs


def retrieve_and_check(question, ensemble_retriever):
    retrieved_documents = ensemble_retriever.invoke(question)
    cleaned_documents = clean_retrieved_documents(retrieved_documents)
    return cleaned_documents


def test_chatbot():
    vectorstore = load_vectorstore()
    retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 4},
        collection_name="pdf_collection",
    )

    qa_chain = create_chain(retriever)
    return qa_chain
