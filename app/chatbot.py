from dotenv import load_dotenv
from typing import List
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from langchain_teddynote.evaluator import OpenAIRelevanceGrader
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from langchain_core.documents import Document
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA

load_dotenv()


class DocumentChatbot:
    def __init__(
        self,
        persist_directory: str,
        documents: List[Document] = None,
        model_name: str = "jhgan/ko-sbert-sts",
    ):
        """
        ì±—ë´‡ ì´ˆê¸°í™”

        Args:
            persist_directory (str): ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ ê²½ë¡œ
            model_name (str): HuggingFace ì„ë² ë”© ëª¨ë¸ ì´ë¦„
        """
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        self.embedding = HuggingFaceEmbeddings(model_name=model_name)

        if documents:
            self.vectorstore = Chroma.from_documents(
                documents=documents,
                embedding_function=self.embedding,
                persist_directory=persist_directory,
            )
        else:
            self.vectorstore = self.load_vectorstore(persist_directory, model_name)

        # ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ë° ë¬¸ì„œ ì¶”ê°€
        self.vectorstore = Chroma.from_documents(
            documents=documents,
            embedding_function=self.embedding,
            persist_directory=persist_directory,
        )

        self.setup_retrievers()
        self.setup_relevance_checker()
        self.setup_chain()

    def setup_chain(self):
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •
        prompt = PromptTemplate.from_template(
            """ë‹¹ì‹ ì€ ì§ˆì˜ì‘ë‹µ ì‘ì—…ì„ ìœ„í•œ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
        ì œê³µëœ ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
        ë‹µì„ ëª¨ë¥´ëŠ” ê²½ìš°ì—ëŠ” ëª¨ë¥¸ë‹¤ê³  ë§ì”€í•´ ì£¼ì„¸ìš”.
        í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.

        # ì§€ì¹¨:
        ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ì •í™•íˆ íŒŒì•…í•˜ê³  ê°€ì¥ ì ì ˆí•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
        - ì§ˆë¬¸ì˜ ë§¥ë½ê³¼ ì§ˆë¬¸ìê°€ ì™œ ì´ ì§ˆë¬¸ì„ í–ˆëŠ”ì§€ ìŠ¤ìŠ¤ë¡œ ë¬¼ì–´ë³´ê³ , ì§ˆë¬¸ì— ëŒ€í•´ ê³ ë¯¼í•œ í›„ ì´í•´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì ì ˆí•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
        2. ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë‚´ìš©(ì§ˆë¬¸ê³¼ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ í•µì‹¬ ë‚´ìš©)ì„ ì„ ë³„í•˜ì—¬ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.
        3. ê°„ê²°í•˜ê³  ë…¼ë¦¬ì ì¸ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”. ë‹µë³€ ì‘ì„± ì‹œ ë‹¨ìˆœíˆ ì„ ë³„í•œ ë‚´ìš©ì„ ë‚˜ì—´í•˜ì§€ ë§ê³ , ë§¥ë½ì— ë§ê²Œ ì¬êµ¬ì„±í•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ë‹¨ì´ ë˜ë„ë¡ í•˜ì„¸ìš”.
        4. ì§ˆë¬¸ì— ëŒ€í•œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆê±°ë‚˜, ë¬¸ì„œëŠ” ê²€ìƒ‰ë˜ì—ˆìœ¼ë‚˜ ê·¸ ë‚´ìš©ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ì—†ëŠ” ê²½ìš° 'ë³´ìœ í•œ ìë£Œì—ì„œ í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë§ì”€í•´ ì£¼ì„¸ìš”.
        5. ë‹µë³€ì„ í•µì‹¬ í¬ì¸íŠ¸ í‘œë¡œ ì‘ì„±í•˜ì„¸ìš”.
        6. ë‹µë³€ì—ëŠ” ë°˜ë“œì‹œ ëª¨ë“  ì¶œì²˜ì™€ í˜ì´ì§€ ë²ˆí˜¸ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
        7. ë‹µë³€ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.
        8. ê°€ëŠ¥í•œ í•œ ìƒì„¸í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
        9. ë‹µë³€ì€ '**ğŸ“š ë¬¸ì„œì—ì„œ ê²€ìƒ‰í•œ ë‚´ìš© ê¸°ë°˜ ë‹µë³€ì…ë‹ˆë‹¤**'ë¡œ ì‹œì‘í•˜ê³  '**ğŸ“Œ ì¶œì²˜**'ë¡œ ëë‚´ì„¸ìš”.
        10. í˜ì´ì§€ ë²ˆí˜¸ëŠ” ì •ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤.

        #Context: 
        {context}

        ###

        #ë‹µë³€ í˜•ì‹ ì˜ˆì‹œ:
        **ğŸ“š ë¬¸ì„œì—ì„œ ê²€ìƒ‰í•œ ë‚´ìš©ê¸°ë°˜ ë‹µë³€ì…ë‹ˆë‹¤**

        (ë‹µë³€ ìš”ì•½)
        (ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ í‘œê°€ ì»¨í…ìŠ¤íŠ¸ì— ìˆëŠ” ê²½ìš° í¬í•¨)
        (ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì´ë¯¸ì§€ ì„¤ëª…ì´ ì»¨í…ìŠ¤íŠ¸ì— ìˆëŠ” ê²½ìš° í¬í•¨)
        (ì§ˆë¬¸ì— ëŒ€í•œ ìƒì„¸ ë‹µë³€)

        **ğŸ“Œ ì¶œì²˜**
        [ì—¬ê¸°ì—ëŠ” íŒŒì¼ëª…(.pdfë§Œ)ê³¼ í˜ì´ì§€ë§Œ ì‘ì„±]

        - íŒŒì¼ëª….pdf, 192ìª½
        - íŒŒì¼ëª….pdf, 192ìª½
        - ...

        ###

        #Question:
        {question}

        #Answer:"""
        )

        # LLM ì„¤ì •
        llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)

        # RetrievalQA ì²´ì¸ ì„¤ì •
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type_kwargs={"prompt": prompt},
            retriever=self.ensemble_retriever,
            return_source_documents=True,
        )

        # ê¸°ì¡´ ì²´ì¸ë„ ìœ ì§€ (í•„ìš”í•œ ê²½ìš°)
        self.chain = (
            {
                "context": RunnableLambda(self.retrieve_and_check),
                "question": RunnablePassthrough(),
            }
            | prompt
            | llm
            | StrOutputParser()
        )

    def setup_retrievers(self):
        # BM25 ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì •
        self.bm25_retriever = BM25Retriever.from_documents(self.vectorstore.get(), k=5)

        # ë²¡í„°ìŠ¤í† ì–´ ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì •
        self.vector_retriever = self.vectorstore.as_retriever(search_kwargs={"k": 5})

        # ì•™ìƒë¸” ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì •
        self.ensemble_retriever = EnsembleRetriever(
            retrievers=[self.bm25_retriever, self.vector_retriever], weights=[0.7, 0.3]
        )

    def setup_relevance_checker(self):
        # Relevance Checker ì„¤ì •
        self.relevance_checker = OpenAIRelevanceGrader(
            ChatOpenAI(model="gpt-4o-mini", temperature=0), target="retrieval-question"
        ).create()

    def retrieve_and_check(self, question, use_checker=True):
        # ë¬¸ì„œ ê²€ìƒ‰
        retrieved_documents = self.ensemble_retriever.invoke(question)

        if not use_checker:
            return retrieved_documents

        # ê´€ë ¨ì„± ì²´í¬ë¥¼ ìœ„í•œ ì…ë ¥ ì¤€ë¹„
        checking_inputs = [
            {"context": doc.page_content, "input": question}
            for doc in retrieved_documents
        ]

        # ê´€ë ¨ì„± ì²´í¬ ìˆ˜í–‰
        checked_results = self.relevance_checker.batch(checking_inputs)

        # ê´€ë ¨ ìˆëŠ” ë¬¸ì„œë§Œ í•„í„°ë§
        filtered_documents = [
            doc
            for doc, result in zip(retrieved_documents, checked_results)
            if result.score == "yes"
        ]

        return filtered_documents

    # ì¶œë ¥ í¬ë§·íŒ…
    def format_response(self, answer: str, sources: List[Document]) -> str:
        """
        ë‹µë³€ê³¼ ì¶œì²˜ë¥¼ í¬ë§·íŒ…í•©ë‹ˆë‹¤.
        """
        response = "**ğŸ“š ë¬¸ì„œì—ì„œ ê²€ìƒ‰í•œ ë‚´ìš© ê¸°ë°˜ ë‹µë³€ì…ë‹ˆë‹¤**\n\n"
        response += f"{answer}\n\n"

        if sources:
            response += "**ğŸ“Œ ì¶œì²˜**\n"
            for source in sources:
                response += f"- {source.metadata.get('source', 'Unknown')}\n"

        return response

    def chat(self, question: str) -> str:
        """
        ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            question (str): ì‚¬ìš©ì ì§ˆë¬¸

        Returns:
            str: ìƒì„±ëœ ë‹µë³€
        """
        try:
            # RetrievalQA ì²´ì¸ì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ ìƒì„±
            result = self.qa_chain({"query": question})

            # ì‘ë‹µ í¬ë§·íŒ…
            return self.format_response(result["result"], result["source_documents"])
        except Exception as e:
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    @staticmethod
    def load_vectorstore(vectordb_path: str, model_name: str = "BAAI/bge-m3"):
        """
        ë²¡í„°ìŠ¤í† ì–´ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
        """
        embedding = HuggingFaceEmbeddings(model_name=model_name)
        return Chroma(persist_directory=vectordb_path, embedding_function=embedding)
